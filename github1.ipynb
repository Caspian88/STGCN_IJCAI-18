{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 10, 2019 15:26\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : data_utils.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, data, stats):\n",
    "        self.__data = data\n",
    "        self.mean = stats['mean']\n",
    "        self.std = stats['std']\n",
    "\n",
    "    def get_data(self, type):\n",
    "        return self.__data[type]\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {'mean': self.mean, 'std': self.std}\n",
    "\n",
    "    def get_len(self, type):\n",
    "        return len(self.__data[type])\n",
    "\n",
    "    def z_inverse(self, type):\n",
    "        return self.__data[type] * self.std + self.mean\n",
    "\n",
    "\n",
    "def seq_gen(len_seq, data_seq, offset, n_frame, n_route, day_slot, C_0=1):\n",
    "    '''\n",
    "    Generate data in the form of standard sequence unit.\n",
    "    :param len_seq: int, the length of target date sequence.\n",
    "    :param data_seq: np.ndarray, source data / time-series.\n",
    "    :param offset:  int, the starting index of different dataset type.\n",
    "    :param n_frame: int, the number of frame within a standard sequence unit,\n",
    "                         which contains n_his = 12 and n_pred = 9 (3 /15 min, 6 /30 min & 9 /45 min).\n",
    "    :param n_route: int, the number of routes in the graph.\n",
    "    :param day_slot: int, the number of time slots per day, controlled by the time window (5 min as default).\n",
    "    :param C_0: int, the size of input channel.\n",
    "    :return: np.ndarray, [len_seq, n_frame, n_route, C_0].\n",
    "    '''\n",
    "    n_slot = day_slot - n_frame + 1\n",
    "\n",
    "    tmp_seq = np.zeros((len_seq * n_slot, n_frame, n_route, C_0))\n",
    "    for i in range(len_seq):\n",
    "        for j in range(n_slot):\n",
    "            sta = (i + offset) * day_slot + j\n",
    "            end = sta + n_frame\n",
    "            tmp_seq[i * n_slot + j, :, :, :] = np.reshape(data_seq[sta:end, :], [n_frame, n_route, C_0])\n",
    "    return tmp_seq\n",
    "\n",
    "\n",
    "def data_gen(file_path, data_config, n_route, n_frame=21, day_slot=288):\n",
    "    '''\n",
    "    Source file load and dataset generation.\n",
    "    :param file_path: str, the file path of data source.\n",
    "    :param data_config: tuple, the configs of dataset in train, validation, test.\n",
    "    :param n_route: int, the number of routes in the graph.\n",
    "    :param n_frame: int, the number of frame within a standard sequence unit,\n",
    "                         which contains n_his = 12 and n_pred = 9 (3 /15 min, 6 /30 min & 9 /45 min).\n",
    "    :param day_slot: int, the number of time slots per day, controlled by the time window (5 min as default).\n",
    "    :return: dict, dataset that contains training, validation and test with stats.\n",
    "    '''\n",
    "    n_train, n_val, n_test = data_config\n",
    "    # generate training, validation and test data\n",
    "    try:\n",
    "        data_seq = pd.read_csv(file_path, header=None).values\n",
    "    except FileNotFoundError:\n",
    "        print(f'ERROR: input file was not found in {file_path}.')\n",
    "\n",
    "    seq_train = seq_gen(n_train, data_seq, 0, n_frame, n_route, day_slot)\n",
    "    seq_val = seq_gen(n_val, data_seq, n_train, n_frame, n_route, day_slot)\n",
    "    seq_test = seq_gen(n_test, data_seq, n_train + n_val, n_frame, n_route, day_slot)\n",
    "\n",
    "    # x_stats: dict, the stats for the train dataset, including the value of mean and standard deviation.\n",
    "    x_stats = {'mean': np.mean(seq_train), 'std': np.std(seq_train)}\n",
    "\n",
    "    # x_train, x_val, x_test: np.array, [sample_size, n_frame, n_route, channel_size].\n",
    "    x_train = z_score(seq_train, x_stats['mean'], x_stats['std'])\n",
    "    x_val = z_score(seq_val, x_stats['mean'], x_stats['std'])\n",
    "    x_test = z_score(seq_test, x_stats['mean'], x_stats['std'])\n",
    "\n",
    "    x_data = {'train': x_train, 'val': x_val, 'test': x_test}\n",
    "    dataset = Dataset(x_data, x_stats)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def gen_batch(inputs, batch_size, dynamic_batch=False, shuffle=False):\n",
    "    '''\n",
    "    Data iterator in batch.\n",
    "    :param inputs: np.ndarray, [len_seq, n_frame, n_route, C_0], standard sequence units.\n",
    "    :param batch_size: int, the size of batch.\n",
    "    :param dynamic_batch: bool, whether changes the batch size in the last batch if its length is less than the default.\n",
    "    :param shuffle: bool, whether shuffle the batches.\n",
    "    '''\n",
    "    len_inputs = len(inputs)\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.arange(len_inputs)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    for start_idx in range(0, len_inputs, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        if end_idx > len_inputs:\n",
    "            if dynamic_batch:\n",
    "                end_idx = len_inputs\n",
    "            else:\n",
    "                break\n",
    "        if shuffle:\n",
    "            slide = idx[start_idx:end_idx]\n",
    "        else:\n",
    "            slide = slice(start_idx, end_idx)\n",
    "\n",
    "        yield inputs[slide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 10, 2019 17:49\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : __init__.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 12, 2019 19:01\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : base_model.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import keras \n",
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras.models\n",
    "import keras.layers\n",
    "from os.path import join as pjoin\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_model(inputs, n_his, Ks, Kt, blocks, keep_prob):\n",
    "    '''\n",
    "    Build the base model.\n",
    "    :param inputs: placeholder.\n",
    "    :param n_his: int, size of historical records for training.\n",
    "    :param Ks: int, kernel size of spatial convolution.\n",
    "    :param Kt: int, kernel size of temporal convolution.\n",
    "    :param blocks: list, channel configs of st_conv blocks.\n",
    "    :param keep_prob: placeholder.\n",
    "    '''\n",
    "    x = inputs[:, 0:n_his, :, :]\n",
    "\n",
    "    # Ko>0: kernel size of temporal convolution in the output layer.\n",
    "    Ko = n_his\n",
    "    # ST-Block\n",
    "    for i, channels in enumerate(blocks):\n",
    "        x = st_conv_block(x, Ks, Kt, channels, i, keep_prob, act_func='GLU')\n",
    "        Ko -= 2 * (Kt - 1)\n",
    "\n",
    "    # Output Layer\n",
    "    if Ko > 1:\n",
    "        y = output_layer(x, Ko, 'output_layer')\n",
    "    else:\n",
    "        raise ValueError(f'ERROR: kernel size Ko must be greater than 1, but received \"{Ko}\".')\n",
    "\n",
    "    tf.add_to_collection(name='copy_loss',\n",
    "                         value=tf.nn.l2_loss(inputs[:, n_his - 1:n_his, :, :] - inputs[:, n_his:n_his + 1, :, :]))\n",
    "    train_loss = tf.nn.l2_loss(y - inputs[:, n_his:n_his + 1, :, :])\n",
    "    single_pred = y[:, 0, :, :]\n",
    "    tf.add_to_collection(name='y_pred', value=single_pred)\n",
    "    return train_loss, single_pred\n",
    "\n",
    "\n",
    "def model_save(sess, global_steps, model_name, save_path='./output/models/'):\n",
    "    '''\n",
    "    Save the checkpoint of trained model.\n",
    "    :param sess: tf.Session().\n",
    "    :param global_steps: tensor, record the global step of training in epochs.\n",
    "    :param model_name: str, the name of saved model.\n",
    "    :param save_path: str, the path of saved model.\n",
    "    :return:\n",
    "    '''\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    prefix_path = saver.save(sess, pjoin(save_path, model_name), global_step=global_steps)\n",
    "    print(f'<< Saving model to {prefix_path} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 12, 2019 17:45\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : layers.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def gconv(x, theta, Ks, c_in, c_out):\n",
    "    '''\n",
    "    Spectral-based graph convolution function.\n",
    "    :param x: tensor, [batch_size, n_route, c_in].\n",
    "    :param theta: tensor, [Ks*c_in, c_out], trainable kernel parameters.\n",
    "    :param Ks: int, kernel size of graph convolution.\n",
    "    :param c_in: int, size of input channel.\n",
    "    :param c_out: int, size of output channel.\n",
    "    :return: tensor, [batch_size, n_route, c_out].\n",
    "    '''\n",
    "    # graph kernel: tensor, [n_route, Ks*n_route]\n",
    "    kernel = tf.get_collection('graph_kernel')[0]\n",
    "    n = tf.shape(kernel)[0]\n",
    "    # x -> [batch_size, c_in, n_route] -> [batch_size*c_in, n_route]\n",
    "    x_tmp = tf.reshape(tf.transpose(x, [0, 2, 1]), [-1, n])\n",
    "    # x_mul = x_tmp * ker -> [batch_size*c_in, Ks*n_route] -> [batch_size, c_in, Ks, n_route]\n",
    "    x_mul = tf.reshape(tf.matmul(x_tmp, kernel), [-1, c_in, Ks, n])\n",
    "    # x_ker -> [batch_size, n_route, c_in, K_s] -> [batch_size*n_route, c_in*Ks]\n",
    "    x_ker = tf.reshape(tf.transpose(x_mul, [0, 3, 1, 2]), [-1, c_in * Ks])\n",
    "    # x_gconv -> [batch_size*n_route, c_out] -> [batch_size, n_route, c_out]\n",
    "    x_gconv = tf.reshape(tf.matmul(x_ker, theta), [-1, n, c_out])\n",
    "    return x_gconv\n",
    "\n",
    "\n",
    "def layer_norm(x, scope):\n",
    "    '''\n",
    "    Layer normalization function.\n",
    "    :param x: tensor, [batch_size, time_step, n_route, channel].\n",
    "    :param scope: str, variable scope.\n",
    "    :return: tensor, [batch_size, time_step, n_route, channel].\n",
    "    '''\n",
    "    _, _, N, C = x.get_shape().as_list()\n",
    "    mu, sigma = tf.nn.moments(x, axes=[2, 3], keep_dims=True)\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        gamma = tf.get_variable('gamma', initializer=tf.ones([1, 1, N, C]))\n",
    "        beta = tf.get_variable('beta', initializer=tf.zeros([1, 1, N, C]))\n",
    "        _x = (x - mu) / tf.sqrt(sigma + 1e-6) * gamma + beta\n",
    "    return _x\n",
    "\n",
    "\n",
    "def temporal_conv_layer(x, Kt, c_in, c_out, act_func='relu'):\n",
    "    '''\n",
    "    Temporal convolution layer.\n",
    "    :param x: tensor, [batch_size, time_step, n_route, c_in].\n",
    "    :param Kt: int, kernel size of temporal convolution.\n",
    "    :param c_in: int, size of input channel.\n",
    "    :param c_out: int, size of output channel.\n",
    "    :param act_func: str, activation function.\n",
    "    :return: tensor, [batch_size, time_step-Kt+1, n_route, c_out].\n",
    "    '''\n",
    "    _, T, n, _ = x.get_shape().as_list()\n",
    "\n",
    "    if c_in > c_out:\n",
    "        w_input = tf.get_variable('wt_input', shape=[1, 1, c_in, c_out], dtype=tf.float32)\n",
    "        tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(w_input))\n",
    "        x_input = tf.nn.conv2d(x, w_input, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    elif c_in < c_out:\n",
    "        # if the size of input channel is less than the output,\n",
    "        # padding x to the same size of output channel.\n",
    "        # Note, _.get_shape() cannot convert a partially known TensorShape to a Tensor.\n",
    "        x_input = tf.concat([x, tf.zeros([tf.shape(x)[0], T, n, c_out - c_in])], axis=3)\n",
    "    else:\n",
    "        x_input = x\n",
    "\n",
    "    # keep the original input for residual connection.\n",
    "    x_input = x_input[:, Kt - 1:T, :, :]\n",
    "\n",
    "    if act_func == 'GLU':\n",
    "        # gated liner unit\n",
    "        wt = tf.get_variable(name='wt', shape=[Kt, 1, c_in, 2 * c_out], dtype=tf.float32)\n",
    "        tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(wt))\n",
    "        bt = tf.get_variable(name='bt', initializer=tf.zeros([2 * c_out]), dtype=tf.float32)\n",
    "        x_conv = tf.nn.conv2d(x, wt, strides=[1, 1, 1, 1], padding='VALID') + bt\n",
    "        return (x_conv[:, :, :, 0:c_out] + x_input) * tf.nn.sigmoid(x_conv[:, :, :, -c_out:])\n",
    "    else:\n",
    "        wt = tf.get_variable(name='wt', shape=[Kt, 1, c_in, c_out], dtype=tf.float32)\n",
    "        tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(wt))\n",
    "        bt = tf.get_variable(name='bt', initializer=tf.zeros([c_out]), dtype=tf.float32)\n",
    "        x_conv = tf.nn.conv2d(x, wt, strides=[1, 1, 1, 1], padding='VALID') + bt\n",
    "        if act_func == 'linear':\n",
    "            return x_conv\n",
    "        elif act_func == 'sigmoid':\n",
    "            return tf.nn.sigmoid(x_conv)\n",
    "        elif act_func == 'relu':\n",
    "            return tf.nn.relu(x_conv + x_input)\n",
    "        else:\n",
    "            raise ValueError(f'ERROR: activation function \"{act_func}\" is not defined.')\n",
    "\n",
    "\n",
    "def spatio_conv_layer(x, Ks, c_in, c_out):\n",
    "    '''\n",
    "    Spatial graph convolution layer.\n",
    "    :param x: tensor, [batch_size, time_step, n_route, c_in].\n",
    "    :param Ks: int, kernel size of spatial convolution.\n",
    "    :param c_in: int, size of input channel.\n",
    "    :param c_out: int, size of output channel.\n",
    "    :return: tensor, [batch_size, time_step, n_route, c_out].\n",
    "    '''\n",
    "    _, T, n, _ = x.get_shape().as_list()\n",
    "\n",
    "    if c_in > c_out:\n",
    "        # bottleneck down-sampling\n",
    "        w_input = tf.get_variable('ws_input', shape=[1, 1, c_in, c_out], dtype=tf.float32)\n",
    "        tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(w_input))\n",
    "        x_input = tf.nn.conv2d(x, w_input, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    elif c_in < c_out:\n",
    "        # if the size of input channel is less than the output,\n",
    "        # padding x to the same size of output channel.\n",
    "        # Note, _.get_shape() cannot convert a partially known TensorShape to a Tensor.\n",
    "        x_input = tf.concat([x, tf.zeros([tf.shape(x)[0], T, n, c_out - c_in])], axis=3)\n",
    "    else:\n",
    "        x_input = x\n",
    "\n",
    "    ws = tf.get_variable(name='ws', shape=[Ks * c_in, c_out], dtype=tf.float32)\n",
    "    tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(ws))\n",
    "    variable_summaries(ws, 'theta')\n",
    "    bs = tf.get_variable(name='bs', initializer=tf.zeros([c_out]), dtype=tf.float32)\n",
    "    # x -> [batch_size*time_step, n_route, c_in] -> [batch_size*time_step, n_route, c_out]\n",
    "    x_gconv = gconv(tf.reshape(x, [-1, n, c_in]), ws, Ks, c_in, c_out) + bs\n",
    "    # x_g -> [batch_size, time_step, n_route, c_out]\n",
    "    x_gc = tf.reshape(x_gconv, [-1, T, n, c_out])\n",
    "    return tf.nn.relu(x_gc[:, :, :, 0:c_out] + x_input)\n",
    "\n",
    "\n",
    "def st_conv_block(x, Ks, Kt, channels, scope, keep_prob, act_func='GLU'):\n",
    "    '''\n",
    "    Spatio-temporal convolutional block, which contains two temporal gated convolution layers\n",
    "    and one spatial graph convolution layer in the middle.\n",
    "    :param x: tensor, batch_size, time_step, n_route, c_in].\n",
    "    :param Ks: int, kernel size of spatial convolution.\n",
    "    :param Kt: int, kernel size of temporal convolution.\n",
    "    :param channels: list, channel configs of a single st_conv block.\n",
    "    :param scope: str, variable scope.\n",
    "    :param keep_prob: placeholder, prob of dropout.\n",
    "    :param act_func: str, activation function.\n",
    "    :return: tensor, [batch_size, time_step, n_route, c_out].\n",
    "    '''\n",
    "    c_si, c_t, c_oo = channels\n",
    "\n",
    "    with tf.variable_scope(f'stn_block_{scope}_in'):\n",
    "        x_s = temporal_conv_layer(x, Kt, c_si, c_t, act_func=act_func)\n",
    "        x_t = spatio_conv_layer(x_s, Ks, c_t, c_t)\n",
    "    with tf.variable_scope(f'stn_block_{scope}_out'):\n",
    "        x_o = temporal_conv_layer(x_t, Kt, c_t, c_oo)\n",
    "    x_ln = layer_norm(x_o, f'layer_norm_{scope}')\n",
    "    return tf.nn.dropout(x_ln, keep_prob)\n",
    "\n",
    "\n",
    "def fully_con_layer(x, n, channel, scope):\n",
    "    '''\n",
    "    Fully connected layer: maps multi-channels to one.\n",
    "    :param x: tensor, [batch_size, 1, n_route, channel].\n",
    "    :param n: int, number of route / size of graph.\n",
    "    :param channel: channel size of input x.\n",
    "    :param scope: str, variable scope.\n",
    "    :return: tensor, [batch_size, 1, n_route, 1].\n",
    "    '''\n",
    "    w = tf.get_variable(name=f'w_{scope}', shape=[1, 1, channel, 1], dtype=tf.float32)\n",
    "    tf.add_to_collection(name='weight_decay', value=tf.nn.l2_loss(w))\n",
    "    b = tf.get_variable(name=f'b_{scope}', initializer=tf.zeros([n, 1]), dtype=tf.float32)\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "\n",
    "def output_layer(x, T, scope, act_func='GLU'):\n",
    "    '''\n",
    "    Output layer: temporal convolution layers attach with one fully connected layer,\n",
    "    which map outputs of the last st_conv block to a single-step prediction.\n",
    "    :param x: tensor, [batch_size, time_step, n_route, channel].\n",
    "    :param T: int, kernel size of temporal convolution.\n",
    "    :param scope: str, variable scope.\n",
    "    :param act_func: str, activation function.\n",
    "    :return: tensor, [batch_size, 1, n_route, 1].\n",
    "    '''\n",
    "    _, _, n, channel = x.get_shape().as_list()\n",
    "\n",
    "    # maps multi-steps to one.\n",
    "    with tf.variable_scope(f'{scope}_in'):\n",
    "        x_i = temporal_conv_layer(x, T, channel, channel, act_func=act_func)\n",
    "    x_ln = layer_norm(x_i, f'layer_norm_{scope}')\n",
    "    with tf.variable_scope(f'{scope}_out'):\n",
    "        x_o = temporal_conv_layer(x_ln, 1, channel, channel, act_func='sigmoid')\n",
    "    # maps multi-channels to one.\n",
    "    x_fc = fully_con_layer(x_o, n, channel, scope)\n",
    "    return x_fc\n",
    "\n",
    "\n",
    "def variable_summaries(var, v_name):\n",
    "    '''\n",
    "    Attach summaries to a Tensor (for TensorBoard visualization).\n",
    "    Ref: https://zhuanlan.zhihu.com/p/33178205\n",
    "    :param var: tf.Variable().\n",
    "    :param v_name: str, name of the variable.\n",
    "    '''\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(f'mean_{v_name}', mean)\n",
    "\n",
    "        with tf.name_scope(f'stddev_{v_name}'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(f'stddev_{v_name}', stddev)\n",
    "\n",
    "        tf.summary.scalar(f'max_{v_name}', tf.reduce_max(var))\n",
    "        tf.summary.scalar(f'min_{v_name}', tf.reduce_min(var))\n",
    "\n",
    "        tf.summary.histogram(f'histogram_{v_name}', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 10, 2019 17:52\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : tester.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import data\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def multi_pred(sess, y_pred, seq, batch_size, n_his, n_pred, step_idx, dynamic_batch=True):\n",
    "    '''\n",
    "    Multi_prediction function.\n",
    "    :param sess: tf.Session().\n",
    "    :param y_pred: placeholder.\n",
    "    :param seq: np.ndarray, [len_seq, n_frame, n_route, C_0].\n",
    "    :param batch_size: int, the size of batch.\n",
    "    :param n_his: int, size of historical records for training.\n",
    "    :param n_pred: int, the length of prediction.\n",
    "    :param step_idx: int or list, index for prediction slice.\n",
    "    :param dynamic_batch: bool, whether changes the batch size in the last one if its length is less than the default.\n",
    "    :return y_ : tensor, 'sep' [len_inputs, n_route, 1]; 'merge' [step_idx, len_inputs, n_route, 1].\n",
    "            len_ : int, the length of prediction.\n",
    "    '''\n",
    "    pred_list = []\n",
    "    for i in gen_batch(seq, min(batch_size, len(seq)), dynamic_batch=dynamic_batch):\n",
    "        # Note: use np.copy() to avoid the modification of source data.\n",
    "        test_seq = np.copy(i[:, 0:n_his + 1, :, :])\n",
    "        step_list = []\n",
    "        for j in range(n_pred):\n",
    "            pred = sess.run(y_pred,\n",
    "                            feed_dict={'data_input:0': test_seq, 'keep_prob:0': 1.0})\n",
    "            if isinstance(pred, list):\n",
    "                pred = np.array(pred[0])\n",
    "            test_seq[:, 0:n_his - 1, :, :] = test_seq[:, 1:n_his, :, :]\n",
    "            test_seq[:, n_his - 1, :, :] = pred\n",
    "            step_list.append(pred)\n",
    "        pred_list.append(step_list)\n",
    "    #  pred_array -> [n_pred, batch_size, n_route, C_0)\n",
    "    pred_array = np.concatenate(pred_list, axis=1)\n",
    "    return pred_array[step_idx], pred_array.shape[1]\n",
    "\n",
    "\n",
    "def model_inference(sess, pred, inputs, batch_size, n_his, n_pred, step_idx, min_va_val, min_val):\n",
    "    '''\n",
    "    Model inference function.\n",
    "    :param sess: tf.Session().\n",
    "    :param pred: placeholder.\n",
    "    :param inputs: instance of class Dataset, data source for inference.\n",
    "    :param batch_size: int, the size of batch.\n",
    "    :param n_his: int, the length of historical records for training.\n",
    "    :param n_pred: int, the length of prediction.\n",
    "    :param step_idx: int or list, index for prediction slice.\n",
    "    :param min_va_val: np.ndarray, metric values on validation set.\n",
    "    :param min_val: np.ndarray, metric values on test set.\n",
    "    '''\n",
    "    x_val, x_test, x_stats = inputs.get_data('val'), inputs.get_data('test'), inputs.get_stats()\n",
    "\n",
    "    if n_his + n_pred > x_val.shape[1]:\n",
    "        raise ValueError(f'ERROR: the value of n_pred \"{n_pred}\" exceeds the length limit.')\n",
    "\n",
    "    y_val, len_val = multi_pred(sess, pred, x_val, batch_size, n_his, n_pred, step_idx)\n",
    "    evl_val = evaluation(x_val[0:len_val, step_idx + n_his, :, :], y_val, x_stats)\n",
    "\n",
    "    # chks: indicator that reflects the relationship of values between evl_val and min_va_val.\n",
    "    chks = evl_val < min_va_val\n",
    "    # update the metric on test set, if model's performance got improved on the validation.\n",
    "    if sum(chks):\n",
    "        min_va_val[chks] = evl_val[chks]\n",
    "        y_pred, len_pred = multi_pred(sess, pred, x_test, batch_size, n_his, n_pred, step_idx)\n",
    "        evl_pred = evaluation(x_test[0:len_pred, step_idx + n_his, :, :], y_pred, x_stats)\n",
    "        min_val = evl_pred\n",
    "    return min_va_val, min_val\n",
    "\n",
    "\n",
    "def model_test(inputs, batch_size, n_his, n_pred, inf_mode, load_path='./output/models/'):\n",
    "    '''\n",
    "    Load and test saved model from the checkpoint.\n",
    "    :param inputs: instance of class Dataset, data source for test.\n",
    "    :param batch_size: int, the size of batch.\n",
    "    :param n_his: int, the length of historical records for training.\n",
    "    :param n_pred: int, the length of prediction.\n",
    "    :param inf_mode: str, test mode - 'merge / multi-step test' or 'separate / single-step test'.\n",
    "    :param load_path: str, the path of loaded model.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    model_path = tf.train.get_checkpoint_state(load_path).model_checkpoint_path\n",
    "\n",
    "    test_graph = tf.Graph()\n",
    "\n",
    "    with test_graph.as_default():\n",
    "        saver = tf.train.import_meta_graph(pjoin(f'{model_path}.meta'))\n",
    "\n",
    "    with tf.Session(graph=test_graph) as test_sess:\n",
    "        saver.restore(test_sess, tf.train.latest_checkpoint(load_path))\n",
    "        print(f'>> Loading saved model from {model_path} ...')\n",
    "\n",
    "        pred = test_graph.get_collection('y_pred')\n",
    "\n",
    "        if inf_mode == 'sep':\n",
    "            # for inference mode 'sep', the type of step index is int.\n",
    "            step_idx = n_pred - 1\n",
    "            tmp_idx = [step_idx]\n",
    "        elif inf_mode == 'merge':\n",
    "            # for inference mode 'merge', the type of step index is np.ndarray.\n",
    "            step_idx = tmp_idx = np.arange(3, n_pred + 1, 3) - 1\n",
    "        else:\n",
    "            raise ValueError(f'ERROR: test mode \"{inf_mode}\" is not defined.')\n",
    "\n",
    "        x_test, x_stats = inputs.get_data('test'), inputs.get_stats()\n",
    "\n",
    "        y_test, len_test = multi_pred(test_sess, pred, x_test, batch_size, n_his, n_pred, step_idx)\n",
    "        evl = evaluation(x_test[0:len_test, step_idx + n_his, :, :], y_test, x_stats)\n",
    "\n",
    "        for ix in tmp_idx:\n",
    "            te = evl[ix - 2:ix + 1]\n",
    "            print(f'Time Step {ix + 1}: MAPE {te[0]:7.3%}; MAE  {te[1]:4.3f}; RMSE {te[2]:6.3f}.')\n",
    "        print(f'Model Test Time {time.time() - start_time:.3f}s')\n",
    "    print('Testing model finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 13, 2019 20:16\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : trainer.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import data\n",
    "import os \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def model_train(inputs, blocks, args, sum_path='./output/tensorboard'):\n",
    "    '''\n",
    "    Train the base model.\n",
    "    :param inputs: instance of class Dataset, data source for training.\n",
    "    :param blocks: list, channel configs of st_conv blocks.\n",
    "    :param args: instance of class argparse, args for training.\n",
    "    '''\n",
    "    n, n_his, n_pred = args.n_route, args.n_his, args.n_pred\n",
    "    Ks, Kt = args.ks, args.kt\n",
    "    batch_size, epoch, inf_mode, opt = args.batch_size, args.epoch, args.inf_mode, args.opt\n",
    "\n",
    "    # Placeholder for model training\n",
    "    x = tf.placeholder(tf.float32, [None, n_his + 1, n, 1], name='data_input')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Define model loss\n",
    "    train_loss, pred = build_model(x, n_his, Ks, Kt, blocks, keep_prob)\n",
    "    tf.summary.scalar('train_loss', train_loss)\n",
    "    copy_loss = tf.add_n(tf.get_collection('copy_loss'))\n",
    "    tf.summary.scalar('copy_loss', copy_loss)\n",
    "\n",
    "    # Learning rate settings\n",
    "    global_steps = tf.Variable(0, trainable=False)\n",
    "    len_train = inputs.get_len('train')\n",
    "    if len_train % batch_size == 0:\n",
    "        epoch_step = len_train / batch_size\n",
    "    else:\n",
    "        epoch_step = int(len_train / batch_size) + 1\n",
    "    # Learning rate decay with rate 0.7 every 5 epochs.\n",
    "    lr = tf.train.exponential_decay(args.lr, global_steps, decay_steps=5 * epoch_step, decay_rate=0.7, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    step_op = tf.assign_add(global_steps, 1)\n",
    "    with tf.control_dependencies([step_op]):\n",
    "        if opt == 'RMSProp':\n",
    "            train_op = tf.train.RMSPropOptimizer(lr).minimize(train_loss)\n",
    "        elif opt == 'ADAM':\n",
    "            train_op = tf.train.AdamOptimizer(lr).minimize(train_loss)\n",
    "        else:\n",
    "            raise ValueError(f'ERROR: optimizer \"{opt}\" is not defined.')\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(pjoin(sum_path, 'train'), sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if inf_mode == 'sep':\n",
    "            # for inference mode 'sep', the type of step index is int.\n",
    "            step_idx = n_pred - 1\n",
    "            tmp_idx = [step_idx]\n",
    "            min_val = min_va_val = np.array([4e1, 1e5, 1e5])\n",
    "        elif inf_mode == 'merge':\n",
    "            # for inference mode 'merge', the type of step index is np.ndarray.\n",
    "            step_idx = tmp_idx = np.arange(3, n_pred + 1, 3) - 1\n",
    "            min_val = min_va_val = np.array([4e1, 1e5, 1e5] * len(step_idx))\n",
    "        else:\n",
    "            raise ValueError(f'ERROR: test mode \"{inf_mode}\" is not defined.')\n",
    "\n",
    "        for i in range(epoch):\n",
    "            start_time = time.time()\n",
    "            for j, x_batch in enumerate(\n",
    "                    gen_batch(inputs.get_data('train'), batch_size, dynamic_batch=True, shuffle=True)):\n",
    "                summary, _ = sess.run([merged, train_op], feed_dict={x: x_batch[:, 0:n_his + 1, :, :], keep_prob: 1.0})\n",
    "                writer.add_summary(summary, i * epoch_step + j)\n",
    "                if j % 50 == 0:\n",
    "                    loss_value = \\\n",
    "                        sess.run([train_loss, copy_loss],\n",
    "                                 feed_dict={x: x_batch[:, 0:n_his + 1, :, :], keep_prob: 1.0})\n",
    "                    print(f'Epoch {i:2d}, Step {j:3d}: [{loss_value[0]:.3f}, {loss_value[1]:.3f}]')\n",
    "            print(f'Epoch {i:2d} Training Time {time.time() - start_time:.3f}s')\n",
    "\n",
    "            start_time = time.time()\n",
    "            min_va_val, min_val = \\\n",
    "                model_inference(sess, pred, inputs, batch_size, n_his, n_pred, step_idx, min_va_val, min_val)\n",
    "\n",
    "            for ix in tmp_idx:\n",
    "                va, te = min_va_val[ix - 2:ix + 1], min_val[ix - 2:ix + 1]\n",
    "                print(f'Time Step {ix + 1}: '\n",
    "                      f'MAPE {va[0]:7.3%}, {te[0]:7.3%}; '\n",
    "                      f'MAE  {va[1]:4.3f}, {te[1]:4.3f}; '\n",
    "                      f'RMSE {va[2]:6.3f}, {te[2]:6.3f}.')\n",
    "            print(f'Epoch {i:2d} Inference Time {time.time() - start_time:.3f}s')\n",
    "\n",
    "            if (i + 1) % args.save == 0:\n",
    "                model_save(sess, global_steps, 'STGCN')\n",
    "        writer.close()\n",
    "    print('Training model finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Time     : Jan. 10, 2019 15:21\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : math_graph.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "\n",
    "def scaled_laplacian(W):\n",
    "    '''\n",
    "    Normalized graph Laplacian function.\n",
    "    :param W: np.ndarray, [n_route, n_route], weighted adjacency matrix of G.\n",
    "    :return: np.matrix, [n_route, n_route].\n",
    "    '''\n",
    "    # d ->  diagonal degree matrix\n",
    "    n, d = np.shape(W)[0], np.sum(W, axis=1)\n",
    "    # L -> graph Laplacian\n",
    "    L = -W\n",
    "    L[np.diag_indices_from(L)] = d\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (d[i] > 0) and (d[j] > 0):\n",
    "                L[i, j] = L[i, j] / np.sqrt(d[i] * d[j])\n",
    "    # lambda_max \\approx 2.0, the largest eigenvalues of L.\n",
    "    lambda_max = eigs(L, k=1, which='LR')[0][0].real\n",
    "    return np.mat(2 * L / lambda_max - np.identity(n))\n",
    "\n",
    "\n",
    "def cheb_poly_approx(L, Ks, n):\n",
    "    '''\n",
    "    Chebyshev polynomials approximation function.\n",
    "    :param L: np.matrix, [n_route, n_route], graph Laplacian.\n",
    "    :param Ks: int, kernel size of spatial convolution.\n",
    "    :param n: int, number of routes / size of graph.\n",
    "    :return: np.ndarray, [n_route, Ks*n_route].\n",
    "    '''\n",
    "    L0, L1 = np.mat(np.identity(n)), np.mat(np.copy(L))\n",
    "\n",
    "    if Ks > 1:\n",
    "        L_list = [np.copy(L0), np.copy(L1)]\n",
    "        for i in range(Ks - 2):\n",
    "            Ln = np.mat(2 * L * L1 - L0)\n",
    "            L_list.append(np.copy(Ln))\n",
    "            L0, L1 = np.matrix(np.copy(L1)), np.matrix(np.copy(Ln))\n",
    "        # L_lsit [Ks, n*n], Lk [n, Ks*n]\n",
    "        return np.concatenate(L_list, axis=-1)\n",
    "    elif Ks == 1:\n",
    "        return np.asarray(L0)\n",
    "    else:\n",
    "        raise ValueError(f'ERROR: the size of spatial kernel must be greater than 1, but received \"{Ks}\".')\n",
    "\n",
    "\n",
    "def first_approx(W, n):\n",
    "    '''\n",
    "    1st-order approximation function.\n",
    "    :param W: np.ndarray, [n_route, n_route], weighted adjacency matrix of G.\n",
    "    :param n: int, number of routes / size of graph.\n",
    "    :return: np.ndarray, [n_route, n_route].\n",
    "    '''\n",
    "    A = W + np.identity(n)\n",
    "    d = np.sum(A, axis=1)\n",
    "    sinvD = np.sqrt(np.mat(np.diag(d)).I)\n",
    "    # refer to Eq.5\n",
    "    return np.mat(np.identity(n) + sinvD * A * sinvD)\n",
    "\n",
    "\n",
    "def weight_matrix(file_path, sigma2=0.1, epsilon=0.5, scaling=True):\n",
    "    '''\n",
    "    Load weight matrix function.\n",
    "    :param file_path: str, the path of saved weight matrix file.\n",
    "    :param sigma2: float, scalar of matrix W.\n",
    "    :param epsilon: float, thresholds to control the sparsity of matrix W.\n",
    "    :param scaling: bool, whether applies numerical scaling on W.\n",
    "    :return: np.ndarray, [n_route, n_route].\n",
    "    '''\n",
    "    try:\n",
    "        W = pd.read_csv(file_path, header=None).values\n",
    "    except FileNotFoundError:\n",
    "        print(f'ERROR: input file was not found in {file_path}.')\n",
    "\n",
    "    # check whether W is a 0/1 matrix.\n",
    "    if set(np.unique(W)) == {0, 1}:\n",
    "        print('The input graph is a 0/1 matrix; set \"scaling\" to False.')\n",
    "        scaling = False\n",
    "\n",
    "    if scaling:\n",
    "        n = W.shape[0]\n",
    "        W = W / 10000.\n",
    "        W2, W_mask = W * W, np.ones([n, n]) - np.identity(n)\n",
    "        # refer to Eq.10\n",
    "        return np.exp(-W2 / sigma2) * (np.exp(-W2 / sigma2) >= epsilon) * W_mask\n",
    "    else:\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Pars Pardazesh\\AppData\\Local\\Temp\\ipykernel_9524\\3942971739.py:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "# @Time     : Jan. 10, 2019 15:15\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : math_utils.py\n",
    "# @Version  : 1.0\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def z_score(x, mean, std):\n",
    "    '''\n",
    "    Z-score normalization function: $z = (X - \\mu) / \\sigma $,\n",
    "    where z is the z-score, X is the value of the element,\n",
    "    $\\mu$ is the population mean, and $\\sigma$ is the standard deviation.\n",
    "    :param x: np.ndarray, input array to be normalized.\n",
    "    :param mean: float, the value of mean.\n",
    "    :param std: float, the value of standard deviation.\n",
    "    :return: np.ndarray, z-score normalized array.\n",
    "    '''\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def z_inverse(x, mean, std):\n",
    "    '''\n",
    "    The inverse of function z_score().\n",
    "    :param x: np.ndarray, input to be recovered.\n",
    "    :param mean: float, the value of mean.\n",
    "    :param std: float, the value of standard deviation.\n",
    "    :return: np.ndarray, z-score inverse array.\n",
    "    '''\n",
    "    return x * std + mean\n",
    "\n",
    "\n",
    "def MAPE(v, v_):\n",
    "    '''\n",
    "    Mean absolute percentage error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :return: int, MAPE averages on all elements of input.\n",
    "    '''\n",
    "    return np.mean(np.abs(v_ - v) / (v + 1e-5))\n",
    "\n",
    "\n",
    "def RMSE(v, v_):\n",
    "    '''\n",
    "    Mean squared error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :return: int, RMSE averages on all elements of input.\n",
    "    '''\n",
    "    return np.sqrt(np.mean((v_ - v) ** 2))\n",
    "\n",
    "\n",
    "def MAE(v, v_):\n",
    "    '''\n",
    "    Mean absolute error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :return: int, MAE averages on all elements of input.\n",
    "    '''\n",
    "    return np.mean(np.abs(v_ - v))\n",
    "\n",
    "\n",
    "def evaluation(y, y_, x_stats):\n",
    "    '''\n",
    "    Evaluation function: interface to calculate MAPE, MAE and RMSE between ground truth and prediction.\n",
    "    Extended version: multi-step prediction can be calculated by self-calling.\n",
    "    :param y: np.ndarray or int, ground truth.\n",
    "    :param y_: np.ndarray or int, prediction.\n",
    "    :param x_stats: dict, paras of z-scores (mean & std).\n",
    "    :return: np.ndarray, averaged metric values.\n",
    "    '''\n",
    "    dim = len(y_.shape)\n",
    "\n",
    "    if dim == 3:\n",
    "        # single_step case\n",
    "        v = z_inverse(y, x_stats['mean'], x_stats['std'])\n",
    "        v_ = z_inverse(y_, x_stats['mean'], x_stats['std'])\n",
    "        return np.array([MAPE(v, v_), MAE(v, v_), RMSE(v, v_)])\n",
    "    else:\n",
    "        # multi_step case\n",
    "        tmp_list = []\n",
    "        # y -> [time_step, batch_size, n_route, 1]\n",
    "        y = np.swapaxes(y, 0, 1)\n",
    "        # recursively call\n",
    "        for i in range(y_.shape[0]):\n",
    "            tmp_res = evaluation(y[i], y_[i], x_stats)\n",
    "            tmp_list.append(tmp_res)\n",
    "        return np.concatenate(tmp_list, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_route N_ROUTE] [--n_his N_HIS]\n",
      "                             [--n_pred N_PRED] [--batch_size BATCH_SIZE]\n",
      "                             [--epoch EPOCH] [--save SAVE] [--ks KS] [--kt KT]\n",
      "                             [--lr LR] [--opt OPT] [--graph GRAPH]\n",
      "                             [--inf_mode INF_MODE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=\"c:\\Users\\Pars Pardazesh\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-6764CtZhYHY8O96m.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# @Time     : Jan. 02, 2019 22:17\n",
    "# @Author   : Veritas YIN\n",
    "# @FileName : main.py\n",
    "# @Version  : 1.0\n",
    "# @Project  : Orion\n",
    "# @IDE      : PyCharm\n",
    "# @Github   : https://github.com/VeritasYin/Project_Orion\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub \n",
    "import tensorflow_datasets\n",
    "\n",
    "import data \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import keras.models\n",
    "import os\n",
    "import keras.layers\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n_route', type=int, default=228)\n",
    "parser.add_argument('--n_his', type=int, default=12)\n",
    "parser.add_argument('--n_pred', type=int, default=9)\n",
    "parser.add_argument('--batch_size', type=int, default=50)\n",
    "parser.add_argument('--epoch', type=int, default=50)\n",
    "parser.add_argument('--save', type=int, default=10)\n",
    "parser.add_argument('--ks', type=int, default=3)\n",
    "parser.add_argument('--kt', type=int, default=3)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--opt', type=str, default='RMSProp')\n",
    "parser.add_argument('--graph', type=str, default='default')\n",
    "parser.add_argument('--inf_mode', type=str, default='merge')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(f'Training configs: {args}')\n",
    "\n",
    "n, n_his, n_pred = args.n_route, args.n_his, args.n_pred\n",
    "Ks, Kt = args.ks, args.kt\n",
    "# blocks: settings of channel size in st_conv_blocks / bottleneck design\n",
    "blocks = [[1, 32, 64], [64, 32, 128]]\n",
    "\n",
    "# Load wighted adjacency matrix W\n",
    "if args.graph == 'default':\n",
    "    W = weight_matrix(pjoin('./dataset', f'PeMSD7_W_{n}.csv'))\n",
    "else:\n",
    "    # load customized graph weight matrix\n",
    "    W = weight_matrix(pjoin('./dataset', args.graph))\n",
    "\n",
    "# Calculate graph kernel\n",
    "L = scaled_laplacian(W)\n",
    "# Alternative approximation method: 1st approx - first_approx(W, n).\n",
    "Lk = cheb_poly_approx(L, Ks, n)\n",
    "tf.add_to_collection(name='graph_kernel', value=tf.cast(tf.constant(Lk), tf.float32))\n",
    "\n",
    "# Data Preprocessing\n",
    "data_file = f'PeMSD7_V_{n}.csv'\n",
    "n_train, n_val, n_test = 34, 5, 5\n",
    "PeMS = data_gen(pjoin('./dataset', data_file), (n_train, n_val, n_test), n, n_his + n_pred)\n",
    "print(f'>> Loading dataset with Mean: {PeMS.mean:.2f}, STD: {PeMS.std:.2f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_train(PeMS, blocks, args)\n",
    "    model_test(PeMS, PeMS.get_len('test'), n_his, n_pred, args.inf_mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
